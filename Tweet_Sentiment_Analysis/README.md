# Machine Learning - Tweet Sentiment Analysis üóØÔ∏è 

- **Category:** Machine Learning 
- **Course:** Machine Learning (CS-433)
- **Date:** Fall 2024
- **Language:** English

---

## üìå Overview

This project focused on **predicting tweet sentiments** (positive
or negative) using both traditional machine learning methods and
advanced transformer models. Given the noisy and informal nature
of tweets, the challenge was to develop models that could
effectively handle short text, slang, and varying writing styles.

The workflow included:

- **Baseline Models:** Implemented traditional machine learning 
approaches such as Logistic Regression and Support Vector Machines
to establish initial performance benchmarks.
- **Advanced Models:** Leveraged BERT and BERTweet, fine-tuning
these deep learning models to better understand contextual nuances
in tweets.
- **Performance Optimization:** Compared accuracy and F1 scores
across different models to quantify improvements.

#

## üõ†Ô∏è Tools Used

- **Programming Language:** Python
- **IDE:** Visual Studio Code, Google Colab, Jupyter Notebook

#

## Table of Contents

1. [Project Contents](#project-contents)
   - [Base Model](#base-model)
   - [Advanced Model](#advanced-model)
   - [Hyperparameter Tuning](#hyperparameter-tuning)
2. [Generated Results](#generated-results)
3. [Acknowledgments](#acknowledgments)

#

## Project Contents

A comprehensive report detailing the methodology, results, and conclusions of this project is included as a PDF in the root directory.

The directory contains a `twitter.datasets.zip` file, which should be unzipped for use.

### Base Model

The base model implementation includes the following files:

- **`main.ipynb`**: Implements the pipeline for Logistic Regression and SVM, covering data cleaning, feature extraction with TF-IDF, model training, and evaluation metrics.
- **`HP_Tuning.ipynb`**: Performs hyperparameter tuning and tests input data. Key hyperparameters for Logistic Regression and SVM are optimized using grid or random search.
- **`helpers.py`**: Provides utilities for data preprocessing (e.g., cleaning, tokenization, lemmatization), vocabulary building, and GloVe embedding integration.
- **`en-80k.txt`**: A dictionary file used for spell correction during preprocessing, containing terms and their frequencies.
- **`submission.csv`**: Stores sentiment predictions generated by the baseline models, formatted for submission.

#### Dependencies

Before running the base model, ensure the following libraries are installed:

```
!pip install numpy nltk symspellpy scikit-learn contractions
```

#### Instructions for Using the Base Model

1. **Run Notebooks**:

   - Open `main.ipynb` in Jupyter Notebook.
   - Specify the paths for training and testing datasets within the notebook:
     ```
     pos_file = "twitter-datasets/train_pos_full.txt"
     neg_file = "twitter-datasets/train_neg_full.txt"
     test_file = "twitter-datasets/test_data.txt"
     ```
   - Execute the cells sequentially to preprocess the data, train the machine learning model, and evaluate its performance. Each step of the pipeline is clearly documented within the notebook.

2. **Save Predictions**: The generated predictions will be saved in `submission.csv`.

### Advanced Model

The advanced model repository includes the following files:

- **`BERT_model.ipynb`**: Implements the fine-tuning of the BERT model for sentiment analysis.
- **`BERTweet_model.ipynb`**: Implements the fine-tuning of the BERTweet model for analyzing social media text.
- **`Model_tuning.ipynb`**: Facilitates hyperparameter tuning for both BERT and BERTweet models. Uncomment the appropriate line in the notebook to select the desired model:
  ```
  model_ckpt = "bert-base-uncased" 
  # model_ckpt = "vinai/bertweet-base"
  ```

#### Dependencies

The code was developed in the \*\*Google Colab\*\* virtual environment, which provides a cloud-based platform for executing

Python scripts. Before running the notebook, it is essential to install the required packages to ensure all dependencies

are available.

The first code cell of each notebook is executed to install the necessary libraries.

```
!pip install -U transformers
!pip install -U accelerate
!pip install -U datasets
!pip install -U bertviz
!pip install -U umap-learn
!pip install -U seaborn
!pip install -U emoji
```

#### Instructions for File Paths and Saving the Model

1. **Specify File Paths for Training and Testing Datasets**:

   Replace the placeholders with the correct file paths to the training and testing datasets. For example:

   ```
   train_neg_path = "drive/train_neg_full.txt"
   train_pos_path = "drive/train_pos_full.txt"
   test_path = "drive/test_data.txt"
   ```

2. **Model Saving**:

   After training the model, save it to the desired directory:

   ```
   trainer.save_model("bert-model")
   ```

3. **Saving Predictions to a CSV File**:

   After generating predictions using the fine-tuned model, save the results to a `.csv` file in the desired directory:

   ```
   output_path = "BERT_predictions.csv" # Define the path for the output file
   ```

## Hyperparameter Tuning

- **Baseline Models**: Use `HP_Tuning.ipynb` to explore and fine-tune hyperparameters for Logistic Regression and SVM. The results of different tests are commented within the notebook.

- **Advanced Models**: Use `Model_tuning.ipynb` to adjust hyperparameters for either BERT or BERTweet. Ensure that the desired model is selected by uncommenting the corresponding line:

  ```
  model_ckpt = "bert-base-uncased"
  # model_ckpt = "vinai/bertweet-base"
  ```

## Generated Results

### Baseline Model:

- `submission.csv`: Contains predictions generated by the trained model on the test set.
- **Accuracy**: 0.772 | **F1 Score**: 0.785

### Advanced Model:

- `BERT_predictions.csv`: Contains predictions generated using the **BERT** model.
- `BERTweet_predictions.csv`: Contains predictions generated using the **BERTweet** model.
- `best_predictions.csv`: Represents the predictions from the best-performing model (**BERTweet**) that were submitted to the test site on **AI Crowd**. Note that exact results may vary due to variations in dataset splitting.
- **BERT Accuracy**: 0.894 | **F1 Score**: 0.894
- **BERTweet Accuracy**: 0.912 | **F1 Score**: 0.913

The **BERTweet model** ranked **7th out of 42** on the [AI Crowd leaderboard](https://www.aicrowd.com/challenges/epfl-ml-text-classification/leaderboards).

## Acknowledgments

This project leverages:

- **SymSpell** for spelling correction.
- **GloVe embeddings** for semantic vectorization.
- **NLTK** for text processing.
- **Transformers** Library for advanced model fine-tuning.

#

## üë∑‚Äç‚ôÇÔ∏è Project Members

- Ali Elkilesly 
- Selim Sherif
- Roy Turk 

#
